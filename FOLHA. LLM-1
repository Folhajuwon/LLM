{"cells":[{"cell_type":"markdown","source":["#Installing the required library"],"metadata":{"id":"Jdd9po4TxMHE"}},{"cell_type":"markdown","metadata":{"id":"c_X7IchbITHB"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":84,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CV0irrcuITHC","executionInfo":{"status":"ok","timestamp":1714669211723,"user_tz":-60,"elapsed":7537,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"90acaa45-3a4d-42e9-b0b7-e0aca4292026"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"]}],"source":["# Installing Library\n","!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"markdown","source":["#This code imports the required libraries and initializes a DistilBERT tokenizer and model for sequence classification.\n","\n","#This code initializes a DistilBERT tokenizer and model for sequence classification using the pre-trained model 'distilbert-base-uncased-finetuned-sst-2-english'"],"metadata":{"id":"2j__zDbIxu4k"}},{"cell_type":"code","source":["# Importing required libraries\n","import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n"],"metadata":{"id":"EdwogatiPa8T","executionInfo":{"status":"ok","timestamp":1714669224372,"user_tz":-60,"elapsed":3624,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}}},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["# This code explains how the the Hugging Face Transformers library is utilised to create a sentiment analysis pipeline using the pre-trained DistilBERT model fine-tuned on the SST-2 dataset for sentiment analysis. The code demonstrates how to use the pipeline to analyze the sentiment of a given text input."],"metadata":{"id":"ShFmqT6-yU5n"}},{"cell_type":"code","execution_count":86,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNpXd49LITHD","executionInfo":{"status":"ok","timestamp":1714669231035,"user_tz":-60,"elapsed":1263,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"3e0945ce-d10b-4da8-a31d-ded228aef987"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9956169128417969}]"]},"metadata":{},"execution_count":86}],"source":["# Importing Pipelines and Models\n","from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\", model =\"distilbert-base-uncased-finetuned-sst-2-english\")\n","classifier(\n","    [\n","        \"I've been waiting for this course my whole life.\"\n","    ]\n",")"]},{"cell_type":"markdown","source":["#In sentiment analysis, the goal is to classify the sentiment expressed in a piece of text. In this code, the classifier function is classifying the sentiment of the input sentences (\"the pizza is not that great.\" and \"but i think i like it!\") into positive, negative, or neutral categories based on the overall sentiment conveyed by the text."],"metadata":{"id":"hDn6mviNzFfG"}},{"cell_type":"code","execution_count":87,"metadata":{"executionInfo":{"status":"ok","timestamp":1714669237137,"user_tz":-60,"elapsed":692,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"1bea78de-637e-43b4-db4a-c61500b8e395","colab":{"base_uri":"https://localhost:8080/"},"id":"LWJkRmuWNLeM"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'NEGATIVE', 'score': 0.9997463822364807},\n"," {'label': 'POSITIVE', 'score': 0.999846339225769}]"]},"metadata":{},"execution_count":87}],"source":["# Classifying sentiment\n","classifier(\n","    [\n","        \"the pizza is not that great.\",\n","        \"but i think i like it!\",\n","    ]\n",")"]},{"cell_type":"markdown","source":["# Another sentiment analysis pipeline from the Transformers library is used, called \"nlptown/bert-base-multilingual-uncased-sentiment\". This model is also capable of analyzing the sentiment of input text in different languages and classifying it into predefined categories such as positive, negative, or neutral."],"metadata":{"id":"R_EFgOD0zsV4"}},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714669264605,"user_tz":-60,"elapsed":6983,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"06b15601-a21b-42d2-f221-96f5479ce4e0","id":"vbq6VuSHr9-F"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': '5 stars', 'score': 0.6576533913612366}]"]},"metadata":{},"execution_count":88}],"source":["# Import Pipelines and Models\n","from transformers import pipeline\n","\n","# Classifying sentiments\n","classifier = pipeline(\"sentiment-analysis\", model =\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","classifier(\n","    [\n","        \"I've been waiting for this course my whole life.\"\n","    ]\n",")"]},{"cell_type":"code","execution_count":89,"metadata":{"executionInfo":{"status":"ok","timestamp":1714669269128,"user_tz":-60,"elapsed":244,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"ab602e9d-6f43-426c-b158-874aa9102b43","colab":{"base_uri":"https://localhost:8080/"},"id":"-REo1_g_uK80"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': '4 stars', 'score': 0.4330196976661682}]"]},"metadata":{},"execution_count":89}],"source":["\n","classifier(\n","    [\n","        \"J'aime la nourriture\",\n","    ]\n",")"]},{"cell_type":"code","execution_count":90,"metadata":{"executionInfo":{"status":"ok","timestamp":1714669271929,"user_tz":-60,"elapsed":394,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"74c5ec6b-509c-41a0-c848-ca69ee8dfbcb","colab":{"base_uri":"https://localhost:8080/"},"id":"XZLX-K32vHik"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': '5 stars', 'score': 0.5941194891929626}]"]},"metadata":{},"execution_count":90}],"source":["\n","classifier(\n","    [\n","        \"amo la comida\",\n","    ]\n",")"]},{"cell_type":"code","execution_count":91,"metadata":{"executionInfo":{"status":"ok","timestamp":1714669276875,"user_tz":-60,"elapsed":233,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"13eaa76b-a90d-492c-ced5-bce269fdf7b7","colab":{"base_uri":"https://localhost:8080/"},"id":"VQprqKZlvWqu"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': '5 stars', 'score': 0.6904445290565491}]"]},"metadata":{},"execution_count":91}],"source":["\n","classifier(\n","    [\n","        \"i love the food\",\n","    ]\n",")"]},{"cell_type":"markdown","source":["#This code is used in natural language processing (NLP) tasks to load a pre-trained tokenizer model for further processing of text data. Here, AutoTokenizer class is used to automatically select the appropriate tokenizer based on the provided checkpoint name which is a pre-trained DistilBERT model fine-tuned on the SST-2 (Stanford Sentiment Treebank) dataset for English language."],"metadata":{"id":"OS2Jt7n20vNJ"}},{"cell_type":"code","execution_count":92,"metadata":{"id":"-UNtQ9W6ITHD","executionInfo":{"status":"ok","timestamp":1714669281466,"user_tz":-60,"elapsed":300,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}}},"outputs":[],"source":["# Load pre-trained tokenizer model\n","from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"markdown","source":["#Tokenization Process:\n","This code tokenizes a list of raw text sentences using the tokenizer object. It first defines a list of raw inputs (raw_inputs) containing the sentences to be tokenized. Then, it uses the tokenizer object to tokenize these raw inputs. The tokenization process involves converting the raw text sentences into sequences of tokens, which are numerical representations of the words in the sentences. The tokenized inputs are then processed with padding and truncation, ensuring that all sequences have the same length for batch processing. Finally, the tokenized inputs are returned as PyTorch tensors (inputs) for further processing, such as training a deep learning model."],"metadata":{"id":"X8S6X71t1hjS"}},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oe7xMOauITHE","executionInfo":{"status":"ok","timestamp":1714669289237,"user_tz":-60,"elapsed":257,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"6e05b30e-34cb-41d4-e43d-48504cff2462"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 2023, 2607, 2026, 2878, 2166,\n","         1012,  102],\n","        [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102,    0,    0,    0,    0,\n","            0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"]}],"source":["# Tokenization Process\n","raw_inputs = [\n","    \"I've been waiting for this course my whole life.\",\n","    \"I hate this so much!\",\n","]\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(inputs)"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"status":"ok","timestamp":1714669296812,"user_tz":-60,"elapsed":266,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"2d82d48c-5162-4d33-b121-0a5c98f499ca","colab":{"base_uri":"https://localhost:8080/"},"id":"tmI-UvkWW2m0"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[ 101, 1045, 2123, 2102, 2428, 2066, 2023, 2607, 2026, 2878, 2166, 1012,\n","          102,    0],\n","        [ 101, 1045, 2293, 2009, 2061, 2172, 1010, 1045, 2097, 2025, 2079, 2009,\n","         2153,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"]}],"source":["raw_inputs = [\n","    \"I dont really like this course my whole life.\",\n","    \"I love it so much, i will not do it again\",\n","]\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(inputs)"]},{"cell_type":"markdown","source":["#This code imports the AutoModel class from the transformers library and uses it to load a pre-trained model. The model is loaded from \"distilbert-base-uncased-finetuned-sst-2-english\"."],"metadata":{"id":"G7B_XZDm2PsX"}},{"cell_type":"code","execution_count":95,"metadata":{"id":"qY0pyfmpITHE","executionInfo":{"status":"ok","timestamp":1714669301239,"user_tz":-60,"elapsed":664,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}}},"outputs":[],"source":["# Import the AutoModel class from the transformers library\n","# Specify the name of the pre-trained model checkpoint\n","# Load the pre-trained model using the AutoModel class\n","\n","from transformers import AutoModel\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModel.from_pretrained(checkpoint)"]},{"cell_type":"markdown","source":["#prediction\n","This code shows that the model is making predictions based on the inputs provided. The outputs variable likely contains the predictions made by the model, and the last hidden state attribute of outputs indicates the final hidden states of the model, which are often used for tasks such as sequence labeling, text classification, or language generation."],"metadata":{"id":"Wc8wkjYE265B"}},{"cell_type":"code","execution_count":96,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxF6Ofd1ITHE","executionInfo":{"status":"ok","timestamp":1714669306875,"user_tz":-60,"elapsed":217,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"25fb1419-2f97-4487-b5d1-9e2687964767"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 14, 768])\n"]}],"source":["# Predictions based on the inputs\n","outputs = model(**inputs)\n","print(outputs.last_hidden_state.shape)"]},{"cell_type":"markdown","source":["#This code demonstrates the use of a pre-trained sequence classification model from the Hugging Face Transformers library\n","\n","\n"],"metadata":{"id":"VbPqSuYw3eih"}},{"cell_type":"code","execution_count":97,"metadata":{"id":"5QaFSmIwITHF","executionInfo":{"status":"ok","timestamp":1714669311125,"user_tz":-60,"elapsed":523,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}}},"outputs":[],"source":["# Load a pre-trained model for sequence classification\n","from transformers import AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","outputs = model(**inputs)"]},{"cell_type":"code","execution_count":98,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjr-wPpDITHF","executionInfo":{"status":"ok","timestamp":1714669313279,"user_tz":-60,"elapsed":223,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"fa028aff-fcf5-4996-9455-32e6a68a8053"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2])\n"]}],"source":["# Print the shape of the logits tensor\n","print(outputs.logits.shape)"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sULGPHYOITHF","executionInfo":{"status":"ok","timestamp":1714669315660,"user_tz":-60,"elapsed":9,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"f6682567-df31-4740-b18c-110c6aa15256"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.4553,  0.6467],\n","        [-3.8361,  4.0565]], grad_fn=<AddmmBackward0>)\n","tensor([[-0.4553,  0.6467],\n","        [-3.8361,  4.0565]], grad_fn=<AddmmBackward0>)\n"]}],"source":["# Print the logits (raw output scores) from the model\n","print(outputs.logits)\n","\n","print(outputs.logits)"]},{"cell_type":"markdown","source":["# This calculates the probability distribution over the classes based on the model's output logits and prints the resulting predictions."],"metadata":{"id":"KFyv-Pbh7gF3"}},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAT3nbAMITHF","executionInfo":{"status":"ok","timestamp":1714669321302,"user_tz":-60,"elapsed":225,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"88679f1a-216a-464f-898f-8970c8d97841"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2.4937e-01, 7.5063e-01],\n","        [3.7332e-04, 9.9963e-01]], grad_fn=<SoftmaxBackward0>)\n"]}],"source":["# Softmax prediction\n","\n","import torch\n","\n","predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","print(predictions)"]},{"cell_type":"markdown","source":["# This dictionary maps integer indices to the corresponding label names or classes. Each integer index represents a class or label, and the corresponding value is the name or label associated with that class."],"metadata":{"id":"dsl2j-Xw72Mb"}},{"cell_type":"code","execution_count":101,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fMyxPbBMITHF","executionInfo":{"status":"ok","timestamp":1714669325927,"user_tz":-60,"elapsed":214,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"aa5a1f6b-8657-4846-a6b1-1c012806aeb0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'NEGATIVE', 1: 'POSITIVE'}"]},"metadata":{},"execution_count":101}],"source":["model.config.id2label"]},{"cell_type":"markdown","source":["# Training and Fine tuning a model\n"],"metadata":{"id":"yGqUfTxq81hh"}},{"cell_type":"code","execution_count":102,"metadata":{"id":"kp1i25ge8KVT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714669348231,"user_tz":-60,"elapsed":19571,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"821d3a9e-38cb-4a54-f3eb-97af2f38341c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}],"source":["# Installation of neccessary packages\n","!pip install datasets evaluate transformers[sentencepiece]\n","!pip install accelerate\n","# To run the training on TPU, you will need to uncomment the following line:\n","# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"]},{"cell_type":"markdown","source":["# This code is for data preprocessing and dataset loading for fine-tuning a pre-trained BERT model on the MRPC (Microsoft Research Paraphrase Corpus) dataset, which is part of the GLUE (General Language Understanding Evaluation) benchmark."],"metadata":{"id":"pxYfWI7VEYE3"}},{"cell_type":"code","execution_count":103,"metadata":{"id":"ZCKVwgwPAoKR","executionInfo":{"status":"ok","timestamp":1714669357994,"user_tz":-60,"elapsed":5252,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}}},"outputs":[],"source":["\n"," # Importing necessary libraries\n","  # Import function to load dataset\n","from datasets import load_dataset\n"," # Import tokenizer and data collator\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n"," # Import numpy library for numerical operations\n","import numpy as np\n","\n","# Load the MRPC dataset from the GLUE benchmark and Specify the pre-trained BERT model checkpoint\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","# Define a tokenization function to tokenize the input sentences\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","# Tokenize the datasets using the tokenization function\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","\n","\n","# Convert the tokenized datasets to TensorFlow datasets for training\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n","\n","tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=True,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")\n","\n","# Convert the tokenized validation dataset to a TensorFlow dataset\n","tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=False,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")"]},{"cell_type":"markdown","source":[" # This code imports a specific model architecture from the transformers library for the task of sequence classification using TensorFlow."],"metadata":{"id":"h4gMo4pMI09z"}},{"cell_type":"code","execution_count":104,"metadata":{"id":"LrEmY5dOAoKS","executionInfo":{"status":"ok","timestamp":1714669373914,"user_tz":-60,"elapsed":8893,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"a15f0623-9ec1-421f-d06a-f501d8d7db18","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Import model from transformer library\n","from transformers import TFAutoModelForSequenceClassification\n","\n","model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"]},{"cell_type":"code","execution_count":105,"metadata":{"id":"mfD67asYAoKS","outputId":"3760547a-b975-41ef-e8dd-aba3b4c72dd1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714673136115,"user_tz":-60,"elapsed":517156,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["459/459 [==============================] - 3709s 8s/step - loss: 0.6817 - accuracy: 0.6303 - val_loss: 0.6240 - val_accuracy: 0.6838\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf_keras.src.callbacks.History at 0x7aae1b771db0>"]},"metadata":{},"execution_count":105}],"source":["# Compiling the model with Adam optimizer and SparseCategoricalCrossentropy loss function\n","# Metrics for evaluation are set to accuracy\n","\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","\n","model.compile(\n","    optimizer=\"adam\",\n","    loss=SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[\"accuracy\"],\n",")\n"," # Training the model using the training dataset and validating using the validation dataset\n","model.fit(\n","    tf_train_dataset,\n","    validation_data=tf_validation_dataset,\n",")"]},{"cell_type":"markdown","source":["# This code utilizes a learning rate scheduler called PolynomialDecay from the TensorFlow library which adjusts the learning rate during training, gradually decreasing it from an initial value to a specified end value over a certain number of training steps."],"metadata":{"id":"KtOC1nyLLtR1"}},{"cell_type":"code","source":["# Importing a learning rate scheduler"],"metadata":{"id":"UWqQMKtPLsxS","executionInfo":{"status":"ok","timestamp":1714674112287,"user_tz":-60,"elapsed":301,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","execution_count":110,"metadata":{"id":"f27kRvS2AoKS","executionInfo":{"status":"ok","timestamp":1714674113756,"user_tz":-60,"elapsed":313,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}}},"outputs":[],"source":["from tensorflow.keras.optimizers.schedules import PolynomialDecay\n","\n","batch_size = 8\n","num_epochs = 3\n","# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n","# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n","# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n","num_train_steps = len(tf_train_dataset) * num_epochs\n","lr_scheduler = PolynomialDecay(\n","    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",")\n","from tensorflow.keras.optimizers import Adam\n","\n","opt = Adam(learning_rate=lr_scheduler)"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"E8qYK21iAoKT","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1714673816176,"user_tz":-60,"elapsed":8094,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"e1c7a92f-e244-4254-cf4e-822f157a82cf"},"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"error","ename":"ValueError","evalue":"Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x7aae08085f00>","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-108-95244f731cb4>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0;31m# This argument got renamed, we need to support both versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"steps_per_execution\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparent_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m             super().compile(\n\u001b[0m\u001b[1;32m   1496\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/__init__.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m         )\n\u001b[1;32m    333\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;34mf\"Could not interpret optimizer identifier: {identifier}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x7aae08085f00>"]}],"source":["# Import library\n","import tensorflow as tf\n","\n","model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6C5OzwsAoKT","colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"status":"error","timestamp":1714668765954,"user_tz":-60,"elapsed":250,"user":{"displayName":"Habdul Folajhuwon","userId":"18275710145051066159"}},"outputId":"d93fd849-1e76-44e5-aefc-6b0107e45f74"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-83-0365f232cd1b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model using the training dataset and validate using the validation dataset for 3 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_validation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_batch_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3976\u001b[0m         \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3977\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3978\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   3979\u001b[0m                 \u001b[0;34m\"You must compile your model before \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3980\u001b[0m                 \u001b[0;34m\"training/testing. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."]}],"source":["# Train the model using the training dataset and validate using the validation dataset for 3 epochs\n","\n","model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PHCM67_AoKT"},"outputs":[],"source":["# Predictions from the model for the validation dataset and extract logits\n","preds = model.predict(tf_validation_dataset)[\"logits\"]\n","\n","preds = model.predict(tf_validation_dataset)[\"logits\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvEjovAPAoKT","outputId":"0c8fac82-9c44-4190-cfaf-f340d3b2d5fe"},"outputs":[{"data":{"text/plain":["(408, 2) (408,)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# Determine the predicted class for each sample by selecting the index with the highest probabili\n","\n","class_preds = np.argmax(preds, axis=1)\n","\n","# Print the shapes of the predictions array and the predicted class array\n","print(preds.shape, class_preds.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcvN5WV3AoKU","outputId":"8b8c141f-dafa-446b-c16f-34d42696ca6e"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# Importing the 'evaluate' module\n","\n","import evaluate\n","\n","# Loading a specific metric ('mrpc') from the 'glue' dataset using the 'load' function from the 'evaluate' module\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","\n","# Computing the metric score by providing predictions ('class_preds') and references ('raw_datasets[\"validation\"][\"label\"]')\n","metric.compute(predictions=class_preds, references=raw_datasets[\"validation\"][\"label\"])"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/master/course/en/chapter2/section2_pt.ipynb","timestamp":1714655415474}]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}